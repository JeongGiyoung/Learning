{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-5. 뉴스 기사 분류.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# # 필요한 라이브러리"
      ],
      "metadata": {
        "id": "uqXCwJaBGhVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6U1_1J5GYRh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 구글 드라이브 마운트"
      ],
      "metadata": {
        "id": "N5GObmGl7ziS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4rUD_dO71tS",
        "outputId": "7a2b22e8-d87b-44e0-ceee-7507c399548c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 이미지 불러오기"
      ],
      "metadata": {
        "id": "U-0cGvRfaQiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "k3gXQsKhaM6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [다중 분류]\n",
        "  * 이 절에서 로이터 뉴스를 46개의 상호 배타적인 토픽으로 분류하는 신경망을 만들어 보겠습니다. 클래스가 많기 때문에 이 문제는 다중 분류의 예입니다. 각 데이터 포인트가 정확히 하나의 범주로 분류되기 때문에 좀 더 정확히 말하면 단일 레이블 다중 분류 문제입니다. 각 데이터 포인트가 여러 개의 범주(가령, 토픽)에 속할 수 있다면 이런 문제는 다중 레이블 다중 분류의 문제가 됩니다."
      ],
      "metadata": {
        "id": "U8KpW6B4G2sh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # 데이터 불러오기\n",
        "  * num_words=10000\n",
        "    - 가장 자주 등장하는 단어 1만 개로 제한"
      ],
      "metadata": {
        "id": "fGpNl6To1ka9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO52X6cG1Tcc",
        "outputId": "c30e1bd3-79dd-4cc1-9f33-3a6e7ef4dbfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "2121728/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 데이터 형태"
      ],
      "metadata": {
        "id": "tRh7wVvl18AF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ### 피처 데이터"
      ],
      "metadata": {
        "id": "jiB8zHBI_Rui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KHd1bQy2BNr",
        "outputId": "c5644601-0f5f-45bd-8db7-af6659929eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 245,\n",
              " 273,\n",
              " 207,\n",
              " 156,\n",
              " 53,\n",
              " 74,\n",
              " 160,\n",
              " 26,\n",
              " 14,\n",
              " 46,\n",
              " 296,\n",
              " 26,\n",
              " 39,\n",
              " 74,\n",
              " 2979,\n",
              " 3554,\n",
              " 14,\n",
              " 46,\n",
              " 4689,\n",
              " 4329,\n",
              " 86,\n",
              " 61,\n",
              " 3499,\n",
              " 4795,\n",
              " 14,\n",
              " 61,\n",
              " 451,\n",
              " 4329,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ### 레이블 데이터"
      ],
      "metadata": {
        "id": "vX1Y1B2l_XpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels, train_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNhoMThj_aS0",
        "outputId": "830310ae-d83f-4826-a23e-e3a4dd52d169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 3,  4,  3, ..., 25,  3, 25]), (8982,))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 원래 데이터 형태로"
      ],
      "metadata": {
        "id": "BkdbUOdo2DCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = reuters.get_word_index()\n",
        "word_index"
      ],
      "metadata": {
        "id": "ho4P1t1l1j81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "reverse_word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFcPixBk2IOE",
        "outputId": "2bb98e62-12b2-4f23-81cb-0b8fdb7032b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{10996: 'mdbl',\n",
              " 16260: 'fawc',\n",
              " 12089: 'degussa',\n",
              " 8803: 'woods',\n",
              " 13796: 'hanging',\n",
              " 20672: 'localized',\n",
              " 20673: 'sation',\n",
              " 20675: 'chanthaburi',\n",
              " 10997: 'refunding',\n",
              " 8804: 'hermann',\n",
              " 20676: 'passsengers',\n",
              " 20677: 'stipulate',\n",
              " 8352: 'heublein',\n",
              " 20713: 'screaming',\n",
              " 16261: 'tcby',\n",
              " 185: 'four',\n",
              " 1642: 'grains',\n",
              " 20680: 'broiler',\n",
              " 12090: 'wooden',\n",
              " 1220: 'wednesday',\n",
              " 13797: 'highveld',\n",
              " 7593: 'duffour',\n",
              " 20681: '0053',\n",
              " 3914: 'elections',\n",
              " 2563: '270',\n",
              " 3551: '271',\n",
              " 5113: '272',\n",
              " 3552: '273',\n",
              " 3400: '274',\n",
              " 7975: 'rudman',\n",
              " 3401: '276',\n",
              " 3478: '277',\n",
              " 3632: '278',\n",
              " 4309: '279',\n",
              " 9381: 'dormancy',\n",
              " 7247: 'errors',\n",
              " 3086: 'deferred',\n",
              " 20683: 'sptnd',\n",
              " 8805: 'cooking',\n",
              " 20684: 'stratabit',\n",
              " 16262: 'designing',\n",
              " 20685: 'metalurgicos',\n",
              " 13798: 'databank',\n",
              " 20686: '300er',\n",
              " 20687: 'shocks',\n",
              " 7972: 'nawg',\n",
              " 20688: 'tnta',\n",
              " 20689: 'perforations',\n",
              " 2891: 'affiliates',\n",
              " 20690: '27p',\n",
              " 16263: 'ching',\n",
              " 595: 'china',\n",
              " 16264: 'wagyu',\n",
              " 3189: 'affiliated',\n",
              " 16265: 'chino',\n",
              " 16266: 'chinh',\n",
              " 20692: 'slickline',\n",
              " 13799: 'doldrums',\n",
              " 12092: 'kids',\n",
              " 3028: 'climbed',\n",
              " 6693: 'controversy',\n",
              " 20693: 'kidd',\n",
              " 12093: 'spotty',\n",
              " 12639: 'rebel',\n",
              " 9382: 'millimetres',\n",
              " 4007: 'golden',\n",
              " 5689: 'projection',\n",
              " 12094: 'stern',\n",
              " 7903: \"hudson's\",\n",
              " 10066: 'dna',\n",
              " 20695: 'dnc',\n",
              " 20696: 'hodler',\n",
              " 2394: 'lme',\n",
              " 20697: 'insolvancy',\n",
              " 13800: 'music',\n",
              " 1984: 'therefore',\n",
              " 10998: 'dns',\n",
              " 6959: 'distortions',\n",
              " 13801: 'thassos',\n",
              " 20698: 'populations',\n",
              " 8806: 'meteorologist',\n",
              " 43: 'loss',\n",
              " 9383: 'exco',\n",
              " 20813: 'adventist',\n",
              " 16267: 'murchison',\n",
              " 10999: 'locked',\n",
              " 13802: 'kampala',\n",
              " 20699: 'arndt',\n",
              " 1267: 'nakasone',\n",
              " 20700: 'steinweg',\n",
              " 3633: \"india's\",\n",
              " 3029: 'wang',\n",
              " 10067: 'wane',\n",
              " 13803: 'unjust',\n",
              " 13804: 'titanium',\n",
              " 850: 'want',\n",
              " 20701: 'pinto',\n",
              " 16268: \"institutes'\",\n",
              " 7973: 'absolute',\n",
              " 4677: 'travel',\n",
              " 6422: 'cutback',\n",
              " 16269: 'nazmi',\n",
              " 1858: 'modest',\n",
              " 16270: 'shopwell',\n",
              " 20702: 'sedi',\n",
              " 20703: 'adoped',\n",
              " 16271: 'tulis',\n",
              " 20704: '18th',\n",
              " 20705: \"wmc's\",\n",
              " 20706: 'menlo',\n",
              " 11000: 'reiners',\n",
              " 12095: 'farmlands',\n",
              " 20707: 'nonsensical',\n",
              " 20708: 'elisra',\n",
              " 2461: 'welcomed',\n",
              " 20709: 'peup',\n",
              " 16272: \"holiday's\",\n",
              " 20711: 'activating',\n",
              " 16273: 'avondale',\n",
              " 16274: 'interational',\n",
              " 20712: 'welcomes',\n",
              " 16275: 'fip',\n",
              " 11001: 'tailings',\n",
              " 4205: 'fit',\n",
              " 16276: 'lifeline',\n",
              " 1916: 'bringing',\n",
              " 4819: 'fix',\n",
              " 6164: '624',\n",
              " 12096: 'naturalite',\n",
              " 6165: 'wales',\n",
              " 8807: 'fin',\n",
              " 11129: 'fio',\n",
              " 20714: 'ceremenony',\n",
              " 20715: 'sovr',\n",
              " 20716: \"yeo's\",\n",
              " 1788: 'effects',\n",
              " 13805: 'sixteen',\n",
              " 8808: 'undeveloped',\n",
              " 13806: 'glutted',\n",
              " 20717: 'barton',\n",
              " 20718: 'froday',\n",
              " 10089: 'arrow',\n",
              " 11002: 'stabilises',\n",
              " 6960: 'allan',\n",
              " 20719: '374p',\n",
              " 3891: '393',\n",
              " 4008: '392',\n",
              " 4206: '391',\n",
              " 3079: '390',\n",
              " 4550: '397',\n",
              " 6166: '396',\n",
              " 6423: '395',\n",
              " 4207: '394',\n",
              " 6961: '399',\n",
              " 4208: '398',\n",
              " 7595: 'stabilised',\n",
              " 5114: 'smelters',\n",
              " 20720: 'oprah',\n",
              " 20721: 'orginially',\n",
              " 20722: \"tvx's\",\n",
              " 16278: 'ponomarev',\n",
              " 20723: 'enviroment',\n",
              " 20724: \"reeves'\",\n",
              " 8363: 'mason',\n",
              " 1670: 'encourage',\n",
              " 7596: 'adapt',\n",
              " 12776: 'abbott',\n",
              " 13808: 'stamping',\n",
              " 20726: 'colquiri',\n",
              " 11003: 'ambrit',\n",
              " 8353: 'strata',\n",
              " 4821: 'corrects',\n",
              " 11922: 'sandra',\n",
              " 859: 'estimate',\n",
              " 20727: 'universally',\n",
              " 20728: 'chlorine',\n",
              " 16279: 'competes',\n",
              " 10068: 'leiner',\n",
              " 8809: 'ministries',\n",
              " 8810: 'disturbed',\n",
              " 13809: 'competed',\n",
              " 8811: 'juergen',\n",
              " 13810: 'kfw',\n",
              " 11004: 'turben',\n",
              " 9384: 'reintroduced',\n",
              " 20729: 'maladies',\n",
              " 4101: 'chevron',\n",
              " 16280: 'lazere',\n",
              " 8812: 'antilles',\n",
              " 11907: 'dti',\n",
              " 9070: 'specially',\n",
              " 4678: 'bilzerian',\n",
              " 13811: 'bakelite',\n",
              " 20730: 'renovated',\n",
              " 568: 'service',\n",
              " 16281: 'payless',\n",
              " 20731: 'spiegler',\n",
              " 831: 'needed',\n",
              " 16282: 'wigglesworth',\n",
              " 6962: 'master',\n",
              " 13812: 'antonson',\n",
              " 20732: 'genesis',\n",
              " 13813: 'vismara',\n",
              " 20734: 'organically',\n",
              " 20735: \"accords'\",\n",
              " 5940: 'task',\n",
              " 7974: 'positively',\n",
              " 3479: 'feasibility',\n",
              " 6963: 'ahmed',\n",
              " 13814: \"suralco's\",\n",
              " 20736: 'awacs',\n",
              " 16283: 'idly',\n",
              " 20737: 'regulator',\n",
              " 12097: 'pseudorabies',\n",
              " 16284: 'staubli',\n",
              " 8813: 'nzi',\n",
              " 5115: 'feeling',\n",
              " 3127: '275',\n",
              " 20738: '6819',\n",
              " 16285: 'gorman',\n",
              " 8354: 'sustaining',\n",
              " 9385: 'spectrum',\n",
              " 20739: 'consenting',\n",
              " 12098: 'recapitalized',\n",
              " 11562: 'sailed',\n",
              " 7597: 'dozen',\n",
              " 1985: 'affairs',\n",
              " 2253: 'courier',\n",
              " 8355: 'kremlin',\n",
              " 895: 'shipments',\n",
              " 16286: \"aquino's\",\n",
              " 10070: 'committing',\n",
              " 5293: 'sugarcane',\n",
              " 9386: 'diminishing',\n",
              " 16287: 'vexing',\n",
              " 11005: 'simplify',\n",
              " 6167: 'mouth',\n",
              " 7248: 'steinhardt',\n",
              " 8814: 'conceded',\n",
              " 9387: 'bradford',\n",
              " 7976: 'singer',\n",
              " 20740: '5602',\n",
              " 13816: \"1987's\",\n",
              " 4950: 'tech',\n",
              " 6424: 'teck',\n",
              " 20741: 'majv',\n",
              " 666: 'saying',\n",
              " 16477: 'dickey',\n",
              " 20742: 'sweetner',\n",
              " 21149: 'teresa',\n",
              " 20743: 'ulcer',\n",
              " 13817: 'cheaply',\n",
              " 2361: 'thai',\n",
              " 6964: 'orleans',\n",
              " 16290: 'excavator',\n",
              " 6168: 'rico',\n",
              " 12099: 'lube',\n",
              " 13818: 'rick',\n",
              " 4679: 'rich',\n",
              " 13819: 'kerna',\n",
              " 950: 'rice',\n",
              " 4209: 'rica',\n",
              " 5503: 'plate',\n",
              " 16291: 'platt',\n",
              " 8356: 'altogether',\n",
              " 8815: 'jaguar',\n",
              " 20744: 'dynair',\n",
              " 8816: 'patch',\n",
              " 2892: 'ldp',\n",
              " 13820: 'boarded',\n",
              " 16292: 'precluding',\n",
              " 11006: 'clarified',\n",
              " 16293: 'sensitivity',\n",
              " 1511: 'alternative',\n",
              " 11007: 'clarifies',\n",
              " 5116: 'lots',\n",
              " 7598: 'irs',\n",
              " 20745: 'irv',\n",
              " 13821: 'iri',\n",
              " 13822: 'ira',\n",
              " 5690: 'timber',\n",
              " 20746: 'ire',\n",
              " 5219: 'discipline',\n",
              " 1937: 'extend',\n",
              " 3634: 'nature',\n",
              " 16295: \"amb's\",\n",
              " 16296: 'dunhill',\n",
              " 2142: 'extent',\n",
              " 20747: 'restrcitions',\n",
              " 2396: 'heating',\n",
              " 11008: \"mannesmann's\",\n",
              " 20748: 'outsanding',\n",
              " 20749: 'multimillions',\n",
              " 13824: 'sarcinelli',\n",
              " 6694: 'southeastern',\n",
              " 10071: 'eradicate',\n",
              " 9388: 'libyan',\n",
              " 20750: 'foreclosing',\n",
              " 12101: 'maclaine',\n",
              " 20751: 'fra',\n",
              " 353: 'union',\n",
              " 11009: 'frn',\n",
              " 386: 'much',\n",
              " 12102: 'fry',\n",
              " 20752: 'mothball',\n",
              " 10072: 'chlorazepate',\n",
              " 12103: 'dxns',\n",
              " 19981: 'toyko',\n",
              " 20753: 'spit',\n",
              " 16297: '007050',\n",
              " 16298: 'freehold',\n",
              " 13825: 'davy',\n",
              " 11010: 'dave',\n",
              " 12177: 'spie',\n",
              " 10117: 'aguayo',\n",
              " 12104: 'wildcat',\n",
              " 10069: 'fecs',\n",
              " 20754: 'kennan',\n",
              " 16299: 'intal',\n",
              " 9389: 'contingencies',\n",
              " 16551: 'professionally',\n",
              " 16300: 'microbiological',\n",
              " 20756: 'misconstrued',\n",
              " 409: 'k',\n",
              " 20757: 'securitiesd',\n",
              " 16301: 'deferring',\n",
              " 5941: 'kohl',\n",
              " 3030: 'conditioned',\n",
              " 20758: 'fnhb',\n",
              " 16302: \"october's\",\n",
              " 13954: 'memorial',\n",
              " 6965: 'democracies',\n",
              " 27520: 'conformed',\n",
              " 464: 'split',\n",
              " 12105: \"bond's\",\n",
              " 11112: 'thinly',\n",
              " 16515: 'dunkirk',\n",
              " 16303: 'cavanaugh',\n",
              " 13827: \"securities'\",\n",
              " 21345: 'marches',\n",
              " 16304: 'issam',\n",
              " 2020: 'workforce',\n",
              " 12106: 'meinert',\n",
              " 13828: 'boiler',\n",
              " 5294: \"bp's\",\n",
              " 16305: 'torpedoed',\n",
              " 20762: 'indidate',\n",
              " 13829: 'downwardly',\n",
              " 20763: 'viviez',\n",
              " 20764: 'vladiminovich',\n",
              " 16306: 'academic',\n",
              " 20765: 'architecural',\n",
              " 1117: 'corporate',\n",
              " 16307: 'appropriately',\n",
              " 20766: 'teicc',\n",
              " 20767: \"hanover's\",\n",
              " 8817: 'aristech',\n",
              " 20768: 'portrayed',\n",
              " 21383: 'raffineries',\n",
              " 20770: 'hai',\n",
              " 7599: 'hal',\n",
              " 13830: 'ham',\n",
              " 10073: 'han',\n",
              " 20771: 'e15b',\n",
              " 61: 'had',\n",
              " 20772: 'hay',\n",
              " 13831: 'botchwey',\n",
              " 10074: 'haq',\n",
              " 37: 'has',\n",
              " 13832: 'hat',\n",
              " 20773: 'hav',\n",
              " 20774: 'fortin',\n",
              " 8818: 'municipal',\n",
              " 20775: 'osman',\n",
              " 20776: 'fsical',\n",
              " 3480: 'elders',\n",
              " 12107: 'survival',\n",
              " 16308: 'unequivocally',\n",
              " 2519: 'objective',\n",
              " 6695: 'indicative',\n",
              " 10075: 'shadow',\n",
              " 21411: 'riskiness',\n",
              " 20778: 'positiive',\n",
              " 10076: \"american's\",\n",
              " 16309: 'alick',\n",
              " 16310: 'harima',\n",
              " 12108: 'alice',\n",
              " 20779: 'altschul',\n",
              " 16311: 'festivities',\n",
              " 20780: 'medecines',\n",
              " 2942: 'beneficial',\n",
              " 12109: 'yoweri',\n",
              " 13833: 'crowd',\n",
              " 9390: 'crowe',\n",
              " 3553: 'crown',\n",
              " 13679: 'topping',\n",
              " 8819: 'captive',\n",
              " 12110: 'billboard',\n",
              " 6169: 'fiduciary',\n",
              " 3402: 'bottom',\n",
              " 20782: 'plucked',\n",
              " 20783: 'locksmithing',\n",
              " 9391: 'ecopetrol',\n",
              " 24018: 'pipestone',\n",
              " 5505: \"growers'\",\n",
              " 20785: 'borrows',\n",
              " 16312: 'eduard',\n",
              " 13834: 'venpres',\n",
              " 16313: 'bamboo',\n",
              " 13835: 'foolish',\n",
              " 20786: 'uruguyan',\n",
              " 20787: 'officeholders',\n",
              " 20788: 'economiques',\n",
              " 16314: 'aden',\n",
              " 4822: 'maxwell',\n",
              " 4680: 'marshall',\n",
              " 16315: 'honeymoon',\n",
              " 16316: 'administer',\n",
              " 20790: 'shoots',\n",
              " 16317: 'rubbertech',\n",
              " 16318: 'johsen',\n",
              " 10077: 'reciprocity',\n",
              " 13836: 'fabric',\n",
              " 20791: 'suffice',\n",
              " 20792: 'spokemsan',\n",
              " 20793: \"sonora's\",\n",
              " 16319: '5865',\n",
              " 16320: \"systems'\",\n",
              " 20794: 'perfumes',\n",
              " 20795: 'halycon',\n",
              " 20796: 'nonvoting',\n",
              " 7250: 'safeguard',\n",
              " 21538: 'sawdust',\n",
              " 20797: \"else's\",\n",
              " 13837: 'arrays',\n",
              " 20798: 'aza',\n",
              " 20799: 'smasher',\n",
              " 12111: 'complications',\n",
              " 1813: 'pesos',\n",
              " 20800: 'relabelling',\n",
              " 3722: 'passenger',\n",
              " 12112: \"avon's\",\n",
              " 20801: 'megahertz',\n",
              " 10683: 'mirror',\n",
              " 8357: 'minas',\n",
              " 16322: 'bourdain',\n",
              " 20802: 'crownx',\n",
              " 6425: 'eventual',\n",
              " 1207: 'crowns',\n",
              " 1369: 'role',\n",
              " 20803: 'obliges',\n",
              " 16323: 'rolf',\n",
              " 13838: 'vegetative',\n",
              " 20804: 'rolm',\n",
              " 4419: 'roll',\n",
              " 2463: 'intend',\n",
              " 16324: 'palms',\n",
              " 19255: 'denys',\n",
              " 13839: 'transported',\n",
              " 20805: 'moresby',\n",
              " 16325: 'devon',\n",
              " 1351: 'intent',\n",
              " 20806: \"camco's\",\n",
              " 5942: 'variable',\n",
              " 20807: 'transporter',\n",
              " 16326: 'danske',\n",
              " 13840: 'friedhelm',\n",
              " 8358: 'hawker',\n",
              " 17774: \"sand's\",\n",
              " 20808: 'preseving',\n",
              " 12113: '80386',\n",
              " 16328: 'bnls',\n",
              " 19984: 'ordination',\n",
              " 11011: 'overturned',\n",
              " 16329: 'erred',\n",
              " 6696: 'cincinnati',\n",
              " 16710: 'corps',\n",
              " 20809: 'whoever',\n",
              " 16330: 'osp',\n",
              " 13841: 'osr',\n",
              " 12114: 'ost',\n",
              " 16331: 'chair',\n",
              " 5647: '690',\n",
              " 20810: 'grapples',\n",
              " 13842: 'megawatts',\n",
              " 20811: 'photocopiers',\n",
              " 20812: 'sconninx',\n",
              " 2274: 'circumstances',\n",
              " 13843: 'oversight',\n",
              " 20814: \"paradyne's\",\n",
              " 6363: '691',\n",
              " 20815: 'paychecks',\n",
              " 13844: \"stadelmann's\",\n",
              " 3241: 'choice',\n",
              " 11012: 'vastagh',\n",
              " 8820: 'embark',\n",
              " 9392: 'gloomy',\n",
              " 9393: 'stays',\n",
              " 4009: 'exact',\n",
              " 5117: 'minute',\n",
              " 11892: 'kittiwake',\n",
              " 20816: 'picul',\n",
              " 20817: 'skewed',\n",
              " 11013: 'cooke',\n",
              " 10078: 'defaults',\n",
              " 11014: 'reimpose',\n",
              " 9394: 'hindered',\n",
              " 20818: 'lengthened',\n",
              " 16333: 'chopping',\n",
              " 13845: 'mckiernan',\n",
              " 20819: 'collaspe',\n",
              " 7251: 'corazon',\n",
              " 7600: 'antwerp',\n",
              " 13846: 'abdullah',\n",
              " 13847: 'goldston',\n",
              " 442: '300',\n",
              " 20821: 'cassa',\n",
              " 20822: 'casse',\n",
              " 4081: '695',\n",
              " 2979: 'ground',\n",
              " 839: 'boost',\n",
              " 16334: 'azusa',\n",
              " 9395: 'drafted',\n",
              " 4823: '303',\n",
              " 13848: 'climbs',\n",
              " 7601: 'honour',\n",
              " 20823: 'vanderbilt',\n",
              " 3968: '305',\n",
              " 3031: 'address',\n",
              " 8821: 'dwindling',\n",
              " 7252: 'benson',\n",
              " 12115: 'enroll',\n",
              " 501: 'revenues',\n",
              " 12116: 'impacted',\n",
              " 20826: 'queue',\n",
              " 10079: 'accomplished',\n",
              " 7602: 'throughput',\n",
              " 9396: 'influx',\n",
              " 10080: 'stockbuilding',\n",
              " 20827: 'aproximates',\n",
              " 13849: 'petroleo',\n",
              " 16335: 'sistemas',\n",
              " 14053: 'feretti',\n",
              " 5943: 'opposes',\n",
              " 882: 'working',\n",
              " 20829: 'perished',\n",
              " 13850: 'oldham',\n",
              " 20830: '27000',\n",
              " 19245: 'optimize',\n",
              " 20832: 'vigour',\n",
              " 1580: 'opposed',\n",
              " 16336: 'liberalizing',\n",
              " 20833: 'wvz',\n",
              " 20834: 'dampness',\n",
              " 13851: 'approving',\n",
              " 13496: 'sierra',\n",
              " 20835: 'entrepot',\n",
              " 224: 'currency',\n",
              " 1499: 'originally',\n",
              " 20837: 'tindemans',\n",
              " 16337: 'valorem',\n",
              " 477: 'following',\n",
              " 20838: 'fossen',\n",
              " 11016: 'locke',\n",
              " 20839: 'employess',\n",
              " 12117: 'rotberg',\n",
              " 16338: 'parachute',\n",
              " 11017: 'locks',\n",
              " 12255: 'incremental',\n",
              " 16339: 'woolowrth',\n",
              " 20841: 'listens',\n",
              " 7253: 'litre',\n",
              " 3554: 'edouard',\n",
              " 1377: 'ounce',\n",
              " 20843: 'nicanor',\n",
              " 20844: 'sucocitrico',\n",
              " 16340: 'minicomputers',\n",
              " 16341: \"silva's\",\n",
              " 11018: 'restitutions',\n",
              " 16342: 'custer',\n",
              " 2590: '3rd',\n",
              " 10081: 'fueled',\n",
              " 20845: 'trydahl',\n",
              " 11019: 'aice',\n",
              " 12118: 'harmon',\n",
              " 10082: 'conscious',\n",
              " 20846: 'herbicidesand',\n",
              " 20847: 'subdivisions',\n",
              " 20848: \"veslefrikk's\",\n",
              " 11020: 'swollen',\n",
              " 7978: 'pulled',\n",
              " 20849: 'tilney',\n",
              " 203: 'years',\n",
              " 20850: 'structuring',\n",
              " 20851: 'episodes',\n",
              " 16343: 'sportscene',\n",
              " 16344: \"northair's\",\n",
              " 20852: 'jig',\n",
              " 20853: 'jin',\n",
              " 3403: 'jim',\n",
              " 8359: 'troubles',\n",
              " 13852: 'workforces',\n",
              " 2362: 'suspension',\n",
              " 3892: 'troubled',\n",
              " 16345: 'fondiaria',\n",
              " 6697: 'modestly',\n",
              " 12119: 'recipients',\n",
              " 7979: 'civilian',\n",
              " 13853: 'indigenous',\n",
              " 20854: 'overpowering',\n",
              " 1051: 'drilling',\n",
              " 16346: 'sorted',\n",
              " 16347: 'lichtenstein',\n",
              " 20855: 'bedevil',\n",
              " 20856: 'dispite',\n",
              " 16843: 'battleships',\n",
              " 4824: 'instability',\n",
              " 95: 'quarter',\n",
              " 20857: 'salado',\n",
              " 5692: 'honduras',\n",
              " 13855: \"chevron's\",\n",
              " 12273: \"lazere's\",\n",
              " 2660: 'receipt',\n",
              " 8360: 'sponsor',\n",
              " 4825: 'entering',\n",
              " 16349: \"kcbt's\",\n",
              " 19987: 'nowicki',\n",
              " 13856: 'salads',\n",
              " 16351: 'augar',\n",
              " 7980: '797',\n",
              " 7254: '796',\n",
              " 8361: '795',\n",
              " 5295: '794',\n",
              " 5118: '793',\n",
              " 6170: '792',\n",
              " 5296: '791',\n",
              " 4826: '790',\n",
              " 20858: \"nikko's\",\n",
              " 20859: 'unsaleable',\n",
              " 5720: '799',\n",
              " 5693: '798',\n",
              " 2143: 'seriously',\n",
              " 16352: 'trauma',\n",
              " 20860: 'tvbh',\n",
              " 20861: 'macedon',\n",
              " 21906: 'disintegrated',\n",
              " 21909: 'adddition',\n",
              " 2244: 'incentives',\n",
              " 5944: 'complicated',\n",
              " 20864: 'reevaluating',\n",
              " 21921: 'thatching',\n",
              " 7981: 'brasil',\n",
              " 20865: '79p',\n",
              " 4951: 'wrong',\n",
              " 8822: 'initiate',\n",
              " 16353: 'aboard',\n",
              " 7255: 'saving',\n",
              " 8823: 'spoken',\n",
              " 16364: 'parkinson',\n",
              " 65: 'one',\n",
              " 20867: 'ont',\n",
              " 7256: 'concert',\n",
              " 16354: \"boston's\",\n",
              " 13859: 'stifled',\n",
              " 4622: 'types',\n",
              " 20868: 'lingering',\n",
              " 16356: 'surges',\n",
              " 20869: 'hurdman',\n",
              " 16357: 'herds',\n",
              " 14114: 'absorbs',\n",
              " 4681: 'surged',\n",
              " 14211: 'dalkon',\n",
              " 13860: 'crossroads',\n",
              " 20870: 'shakeup',\n",
              " 20871: 'disasterous',\n",
              " 11021: 'illness',\n",
              " 3242: 'turned',\n",
              " 3801: 'locations',\n",
              " 12120: 'tyranite',\n",
              " 13861: 'minesweepers',\n",
              " 7257: 'turner',\n",
              " 20872: 'borough',\n",
              " 12358: 'underlines',\n",
              " 20873: \"bancorporation's\",\n",
              " 20874: 'fashionable',\n",
              " 20875: \"ae's\",\n",
              " 16358: 'dilutions',\n",
              " 9472: 'goodman',\n",
              " 10510: 'unlawfully',\n",
              " 16359: 'mayer',\n",
              " 16360: 'printer',\n",
              " 20877: 'offload',\n",
              " 13862: 'opposite',\n",
              " 738: 'buffer',\n",
              " 9398: 'printed',\n",
              " 16361: 'pequiven',\n",
              " 13863: 'panoche',\n",
              " 20878: 'knowingly',\n",
              " 16362: 'ecusta',\n",
              " 20879: 'thsl',\n",
              " 8825: 'phil',\n",
              " 13864: 'jitters',\n",
              " 16363: 'touche',\n",
              " 20881: 'jittery',\n",
              " 3291: 'friction',\n",
              " 16365: 'fecal',\n",
              " 22068: 'resurgance',\n",
              " 20882: 'heeding',\n",
              " 2363: 'soviets',\n",
              " 16366: 'imagined',\n",
              " 16367: 'transact',\n",
              " 20883: 'califoirnia',\n",
              " 9399: \"chrysler's\",\n",
              " 16368: 'respecitvely',\n",
              " 16369: 'presse',\n",
              " 10084: 'euromarket',\n",
              " 12121: 'guarded',\n",
              " 16371: 'satisfacotry',\n",
              " 20884: 'authroization',\n",
              " 20885: 'simplistic',\n",
              " 20886: 'monde',\n",
              " 4102: 'awaiting',\n",
              " 13865: 'recombinant',\n",
              " 20887: 'refinancement',\n",
              " 20888: 'comserv',\n",
              " 20889: 'kitakyushu',\n",
              " 16372: 'pima',\n",
              " 11022: 'basle',\n",
              " 20891: '6250',\n",
              " 16373: 'choudhury',\n",
              " 8826: 'vision',\n",
              " 20892: 'interruptible',\n",
              " 13866: 'weatherford',\n",
              " 7982: '832',\n",
              " 5694: '833',\n",
              " 4420: '830',\n",
              " 5119: '831',\n",
              " 5297: '836',\n",
              " 4553: '837',\n",
              " 6172: '834',\n",
              " 4952: '835',\n",
              " 22144: 'alarming',\n",
              " 5695: '838',\n",
              " 6173: '839',\n",
              " 20893: '524p',\n",
              " 20894: 'sponsorship',\n",
              " 12122: 'vendex',\n",
              " 20895: \"amsouth's\",\n",
              " 20896: 'kilometer',\n",
              " 10086: 'enjoys',\n",
              " 20897: 'illiberal',\n",
              " 6174: 'punta',\n",
              " 20898: 'punte',\n",
              " 10087: 'girozentrale',\n",
              " 20899: 'missstatements',\n",
              " 10088: 'marietta',\n",
              " 6175: 'awards',\n",
              " 3635: 'concentrated',\n",
              " 20900: '83p',\n",
              " 13867: 'developpement',\n",
              " 13868: 'rhodes',\n",
              " 5696: 'matheson',\n",
              " 20901: '1720',\n",
              " 20902: 'paring',\n",
              " 35: 's',\n",
              " 4953: 'concentrates',\n",
              " 16374: \"can's\",\n",
              " 22183: 'polysaturated',\n",
              " 20903: 'parini',\n",
              " 13869: 'baden',\n",
              " 20904: 'bader',\n",
              " 12123: 'buoyancy',\n",
              " 20905: 'erdem',\n",
              " 16375: 'properites',\n",
              " 20906: 'comparitive',\n",
              " 12124: 'practises',\n",
              " 20907: 'collides',\n",
              " 189: 'west',\n",
              " 20908: 'wess',\n",
              " 13870: 'collided',\n",
              " 20909: 'practised',\n",
              " 20910: \"amalgamated's\",\n",
              " 20911: 'motives',\n",
              " 1378: 'wants',\n",
              " 1273: 'formed',\n",
              " 20912: 'readings',\n",
              " 12125: 'geothermal',\n",
              " 7315: 'tightened',\n",
              " 11023: \"d'or\",\n",
              " 1109: 'former',\n",
              " 20913: 'venezulean',\n",
              " 19935: 'curd',\n",
              " 12126: 'squeezes',\n",
              " 1019: 'newspaper',\n",
              " 817: 'situation',\n",
              " 13871: 'ivey',\n",
              " 3636: 'engaged',\n",
              " 13872: 'dubious',\n",
              " 17061: 'cayacq',\n",
              " 20916: 'cobol',\n",
              " 20917: 'limping',\n",
              " 883: 'technology',\n",
              " 20919: 'koerner',\n",
              " 16376: 'debilitating',\n",
              " 7983: 'verified',\n",
              " 4010: 'otto',\n",
              " 20920: '7770',\n",
              " 16377: 'emulsions',\n",
              " 16378: \"onic's\",\n",
              " 9075: 'slate',\n",
              " 20921: 'wires',\n",
              " 5506: 'edged',\n",
              " 20922: 'assigns',\n",
              " 1341: 'singapore',\n",
              " 20923: 'deflate',\n",
              " 20924: \"strategy's\",\n",
              " 16379: 'walesa',\n",
              " 4554: 'advertisement',\n",
              " 20925: 'luyten',\n",
              " 20926: 'shrortly',\n",
              " 20927: 'corpoartion',\n",
              " 22290: 'preferance',\n",
              " 16380: 'tracking',\n",
              " 13874: 'sunnyvale',\n",
              " 20928: 'colorants',\n",
              " 16381: 'persistently',\n",
              " 16382: \"officers'\",\n",
              " 20929: \"his's\",\n",
              " 367: 'being',\n",
              " 7259: 'divestitures',\n",
              " 20930: 'steamer',\n",
              " 20931: 'rover',\n",
              " 8362: 'grounded',\n",
              " 16383: \"businessmen's\",\n",
              " 16384: 'cyanidation',\n",
              " 20932: 'overthrow',\n",
              " 20933: 'partnerhip',\n",
              " 16385: 'sumt',\n",
              " 8827: 'sums',\n",
              " 16386: 'oelmuehle',\n",
              " 16387: 'unveil',\n",
              " 13875: 'gestures',\n",
              " 20934: 'penta',\n",
              " 2544: 'traffic',\n",
              " 2428: 'preference',\n",
              " 20935: 'sumi',\n",
              " 166: 'world',\n",
              " 9400: 'postal',\n",
              " 16388: 'bced',\n",
              " 12128: 'dornbush',\n",
              " 14215: 'confine',\n",
              " 20936: '2555',\n",
              " 5945: \"zambia's\",\n",
              " 20937: 'superiority',\n",
              " 20938: 'militate',\n",
              " 2395: 'satisfactory',\n",
              " 20939: 'superintendent',\n",
              " 5946: 'tvx',\n",
              " 16389: 'tvt',\n",
              " 6698: 'magma',\n",
              " 20940: 'diving',\n",
              " 15548: 'tvb',\n",
              " 13876: 'seaman',\n",
              " 11025: 'matsunaga',\n",
              " 4827: '919',\n",
              " 5298: '918',\n",
              " 17070: 'refundable',\n",
              " 5947: '914',\n",
              " 7260: '917',\n",
              " 6699: '916',\n",
              " 5507: '911',\n",
              " 4828: '910',\n",
              " 10213: 'restoring',\n",
              " 4555: '912',\n",
              " 20942: 'squabble',\n",
              " 7261: 'retains',\n",
              " 20943: \"partner's\",\n",
              " 5300: 'leadership',\n",
              " 11026: 'graaf',\n",
              " 20944: 'spacelab',\n",
              " 1800: 'thailand',\n",
              " 9402: 'graan',\n",
              " 20945: 'exasperating',\n",
              " 12129: 'hartmarx',\n",
              " 16390: 'frights',\n",
              " 20946: 'niall',\n",
              " 11027: 'johnston',\n",
              " 16391: '91p',\n",
              " 16392: 'sensitively',\n",
              " 6016: 'porsche',\n",
              " 15494: 'prepares',\n",
              " 12130: 'lively',\n",
              " 10686: 'stoppages',\n",
              " 16394: \"associated's\",\n",
              " 12131: 'pivot',\n",
              " 1037: 'series',\n",
              " 24050: 'sese',\n",
              " 7604: 'bubble',\n",
              " 16395: 'trusses',\n",
              " 20949: 'interestate',\n",
              " 20950: 'continents',\n",
              " 20951: 'societal',\n",
              " 28: 'with',\n",
              " 6176: 'pull',\n",
              " 6700: 'rush',\n",
              " 6222: 'monopoly',\n",
              " 20953: 'operationally',\n",
              " 20954: 'dirty',\n",
              " 10090: 'abuses',\n",
              " 7262: 'prudhoe',\n",
              " 5949: 'pulp',\n",
              " 16396: 'rust',\n",
              " 20955: 'hellman',\n",
              " 20956: 'amdec',\n",
              " 16397: 'australasian',\n",
              " 13878: 'watches',\n",
              " 20957: 'hypertension',\n",
              " 20958: \"hemdale's\",\n",
              " 16398: 'formulation',\n",
              " 7605: 'watched',\n",
              " 20959: 'jargon',\n",
              " 13879: 'cream',\n",
              " 9404: 'ideally',\n",
              " 11028: 'ryavec',\n",
              " 20960: 'microoganisms',\n",
              " 13880: 'indemnify',\n",
              " 20961: 'wincenty',\n",
              " 20962: 'waving',\n",
              " 20963: \"multifood's\",\n",
              " 20964: 'midges',\n",
              " 11029: 'natalie',\n",
              " 13881: 'crosbie',\n",
              " 20965: 'posible',\n",
              " 13882: 'omnibus',\n",
              " 20966: 'assetsof',\n",
              " 13883: 'tricks',\n",
              " 16399: 'rs',\n",
              " 20967: 'kilogram',\n",
              " 25363: 'pruning',\n",
              " 13884: 'dyer',\n",
              " 20968: 'dyes',\n",
              " 20969: 'legislatures',\n",
              " 16400: 'scm',\n",
              " 9405: 'sci',\n",
              " 20970: 'riedel',\n",
              " 16401: 'ceramic',\n",
              " 6701: 'unitholders',\n",
              " 13885: 'scb',\n",
              " 20971: 'dn11',\n",
              " 20972: 'conditionality',\n",
              " 13807: \"stock's\",\n",
              " 20973: 'masland',\n",
              " 7606: 'causes',\n",
              " 10091: 'riots',\n",
              " 20974: 'norf',\n",
              " 9406: 'nord',\n",
              " 3893: 'midwest',\n",
              " 13886: 'tamils',\n",
              " 16402: 'ofthe',\n",
              " 3421: \"colombia's\",\n",
              " 11030: '24th',\n",
              " 20975: 'sant',\n",
              " 10092: 'moines',\n",
              " 22577: 'electrotechnical',\n",
              " 24534: 'proceeded',\n",
              " 20976: 'sanz',\n",
              " 13887: 'insufficiently',\n",
              " 20977: 'sang',\n",
              " 5950: 'sand',\n",
              " 16404: 'bracho',\n",
              " 805: 'small',\n",
              " 20978: 'workloads',\n",
              " 6702: 'sank',\n",
              " 20979: 'kemper',\n",
              " 16405: 'abbreviated',\n",
              " 13888: 'quicker',\n",
              " 3802: '199',\n",
              " 3243: '198',\n",
              " 2661: '195',\n",
              " 3080: '194',\n",
              " 4310: '197',\n",
              " 3894: '196',\n",
              " 2850: '191',\n",
              " 2199: '190',\n",
              " 3481: '193',\n",
              " 3350: '192',\n",
              " 582: 'past',\n",
              " 20980: 'fractionation',\n",
              " 20981: 'displays',\n",
              " 3081: 'pass',\n",
              " 202: 'investment',\n",
              " 27062: 'quals',\n",
              " 16406: 'quicken',\n",
              " 20983: \"centronic's\",\n",
              " 20984: 'menswear',\n",
              " 16407: 'clock',\n",
              " 20985: 'teape',\n",
              " 20986: 'teapa',\n",
              " 10093: 'prevailed',\n",
              " 9407: 'hebei',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0, 1, 2는 '패딩', '문서 시작', '사전에 없음'을 위한 인덱스이므로 3을 뺍니다\n",
        "\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
        "decoded_newswire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "stjCoJda2ca8",
        "outputId": "42ca3d79-903e-468c-fb84-9388e9816bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # 데이터를 벡터로 변환\n",
        "  * 레이블을 벡터로 바꾸는 방법은 두 가지입니다. \n",
        "    - 레이블의 리스트를 정수 텐서로 변환하는 것과 \n",
        "    - 원-핫 인코딩을 사용하는 것입니다. \n",
        "      + 원-핫 인코딩이 범주형 데이터에 널리 사용되기 때문에 범주형 인코딩이라고도 부릅니다.\n",
        "      + 원-핫 인코딩에 대한 자세한 설명은 6.1절을 참고하세요. 이 경우 레이블의 원-핫 인코딩은 각 레이블의 인덱스 자리는 1이고 나머지는 모두 0인 벡터입니다:"
      ],
      "metadata": {
        "id": "Dz6Ce-hc2onb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 정수 텐서로 변환"
      ],
      "metadata": {
        "id": "y8GpDgAE6KK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# 훈련 데이터 벡터 변환\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# 테스트 데이터 벡터 변환\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "metadata": {
        "id": "0nwhq2sR3I4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjr_yv0e41dF",
        "outputId": "6951e2b0-b2ae-4940-a9ad-d2ef5e114d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8982, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfvgJxdW45pU",
        "outputId": "3e15ed25-a3d0-4cac-d04d-8c30dc01e0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_x_train = pd.DataFrame(x_train)\n",
        "df_x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cd6BxBvT3Xac",
        "outputId": "e8383101-3a57-455f-9ca4-40971b7db080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      0     1     2     3     4     5     6     7     8     9     ...  9990  \\\n",
              "0      0.0   1.0   1.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   0.0   \n",
              "1      0.0   1.0   1.0   0.0   1.0   1.0   0.0   1.0   1.0   1.0  ...   0.0   \n",
              "2      0.0   1.0   1.0   0.0   1.0   1.0   0.0   1.0   0.0   1.0  ...   0.0   \n",
              "3      0.0   1.0   1.0   0.0   1.0   1.0   1.0   1.0   1.0   0.0  ...   0.0   \n",
              "4      0.0   1.0   1.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   0.0   \n",
              "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
              "8977   0.0   1.0   1.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   0.0   \n",
              "8978   0.0   1.0   1.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   0.0   \n",
              "8979   0.0   1.0   1.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   0.0   \n",
              "8980   0.0   1.0   1.0   0.0   1.0   1.0   1.0   1.0   0.0   1.0  ...   0.0   \n",
              "8981   0.0   1.0   1.0   0.0   1.0   0.0   1.0   1.0   1.0   1.0  ...   0.0   \n",
              "\n",
              "      9991  9992  9993  9994  9995  9996  9997  9998  9999  \n",
              "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
              "8977   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "8978   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "8979   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "8980   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "8981   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "\n",
              "[8982 rows x 10000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15534f03-4938-4f8a-8d1a-6ec679810e81\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>9990</th>\n",
              "      <th>9991</th>\n",
              "      <th>9992</th>\n",
              "      <th>9993</th>\n",
              "      <th>9994</th>\n",
              "      <th>9995</th>\n",
              "      <th>9996</th>\n",
              "      <th>9997</th>\n",
              "      <th>9998</th>\n",
              "      <th>9999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8977</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8978</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8979</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8980</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8981</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8982 rows × 10000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15534f03-4938-4f8a-8d1a-6ec679810e81')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15534f03-4938-4f8a-8d1a-6ec679810e81 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15534f03-4938-4f8a-8d1a-6ec679810e81');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 원핫인코딩"
      ],
      "metadata": {
        "id": "5ZVQQ4VX6HR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "# 훈련 레이블 벡터 변환\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "# 테스트 레이블 벡터 변환\n",
        "one_hot_test_labels = to_one_hot(test_labels)"
      ],
      "metadata": {
        "id": "XR8b8hrh3ehc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUx8ktw050xm",
        "outputId": "b03e1eea-3f79-4cc0-be10-b4e7cd826b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_one_hot_train_labels = pd.DataFrame(one_hot_train_labels)\n",
        "df_one_hot_train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wbT9aFJd51mT",
        "outputId": "76b0421c-dab4-476e-db9a-0c20862b96df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0    1    2    3    4    5    6    7    8    9   ...   36   37   38  \\\n",
              "0     0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "1     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "2     0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "3     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "4     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "8977  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "8978  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "8979  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "8980  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "8981  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "\n",
              "       39   40   41   42   43   44   45  \n",
              "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "...   ...  ...  ...  ...  ...  ...  ...  \n",
              "8977  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "8978  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "8979  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "8980  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "8981  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
              "\n",
              "[8982 rows x 46 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e7a58c4-fcdf-40bf-9b03-a29cdac18ae3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8977</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8978</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8979</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8980</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8981</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8982 rows × 46 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e7a58c4-fcdf-40bf-9b03-a29cdac18ae3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e7a58c4-fcdf-40bf-9b03-a29cdac18ae3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e7a58c4-fcdf-40bf-9b03-a29cdac18ae3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ### 케라스에 내장함수로 원핫인코딩"
      ],
      "metadata": {
        "id": "FGU2Wz5Z6UUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)\n",
        "\n",
        "one_hot_train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSMuNen66X4j",
        "outputId": "f9e8c63e-5974-4d88-c7a4-d80f88f12a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # 모델 구성\n",
        "  * 마지막 Dense 층의 크기가 46입니다. 각 입력 샘플에 대해서 46차원의 벡터를 출력한다는 뜻입니다. 이 벡터의 각 원소(각 차원)은 각기 다른 출력 클래스가 인코딩된 것입니다.\n",
        "  * 마지막 층에 softmax 활성화 함수가 사용되었습니다. MNIST 예제에서 이런 방식을 보았습니다. 각 입력 샘플마다 46개의 출력 클래스에 대한 확률 분포를 출력합니다. \n",
        "    - 즉, 46차원의 출력 벡터를 만들며 output[i]는 어떤 샘플이 클래스 i에 속할 확률입니다. 46개의 값을 모두 더하면 1이 됩니다.\n",
        "    - [softmax 이해](https://wikidocs.net/35476)"
      ],
      "metadata": {
        "id": "On1lNXDC6bqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "metadata": {
        "id": "sCGUDgdC6ooh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## categorical_crossentropy\n",
        "  * 이런 문제에 사용할 최선의 손실 함수는 categorical_crossentropy입니다. 이 함수는 두 확률 분포의 사이의 거리를 측정합니다. 여기에서는 네트워크가 출력한 확률 분포와 진짜 레이블의 분포 사이의 거리입니다. 두 분포 사이의 거리를 최소화하면 진짜 레이블에 가능한 가까운 출력을 내도록 모델을 훈련하게 됩니다."
      ],
      "metadata": {
        "id": "izNu9KPm7Fer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "_kTHGlvW7KyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 훈련 검증\n",
        "  * 훈련 데이터에서 1,000개의 샘플을 따로 떼어서 검증 세트로 사용하겠습니다:"
      ],
      "metadata": {
        "id": "CTZzzOmI7W3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ],
      "metadata": {
        "id": "R9x7mSjW9Kpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0MyixG99LDD",
        "outputId": "fa48d8eb-889f-48af-d9d4-dd3b16943edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "16/16 [==============================] - 2s 57ms/step - loss: 2.5815 - accuracy: 0.5264 - val_loss: 1.6946 - val_accuracy: 0.6530\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 1.3959 - accuracy: 0.7082 - val_loss: 1.2753 - val_accuracy: 0.7200\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 1.0394 - accuracy: 0.7772 - val_loss: 1.1091 - val_accuracy: 0.7650\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.8182 - accuracy: 0.8265 - val_loss: 1.0192 - val_accuracy: 0.7900\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.6465 - accuracy: 0.8634 - val_loss: 0.9530 - val_accuracy: 0.8000\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.5139 - accuracy: 0.8910 - val_loss: 0.9258 - val_accuracy: 0.8110\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.4120 - accuracy: 0.9151 - val_loss: 0.9118 - val_accuracy: 0.8130\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.3359 - accuracy: 0.9272 - val_loss: 0.8769 - val_accuracy: 0.8230\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.2790 - accuracy: 0.9382 - val_loss: 0.8722 - val_accuracy: 0.8200\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.2341 - accuracy: 0.9453 - val_loss: 0.9008 - val_accuracy: 0.8190\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.2057 - accuracy: 0.9489 - val_loss: 0.9228 - val_accuracy: 0.8180\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.1779 - accuracy: 0.9509 - val_loss: 0.9490 - val_accuracy: 0.8090\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.1618 - accuracy: 0.9548 - val_loss: 0.9433 - val_accuracy: 0.8130\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.1460 - accuracy: 0.9568 - val_loss: 0.9733 - val_accuracy: 0.8080\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.1410 - accuracy: 0.9558 - val_loss: 0.9629 - val_accuracy: 0.8250\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.1278 - accuracy: 0.9575 - val_loss: 1.0188 - val_accuracy: 0.8080\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.1212 - accuracy: 0.9579 - val_loss: 1.0493 - val_accuracy: 0.8130\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.1153 - accuracy: 0.9588 - val_loss: 1.0847 - val_accuracy: 0.8010\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.1124 - accuracy: 0.9585 - val_loss: 1.1207 - val_accuracy: 0.8080\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.1111 - accuracy: 0.9573 - val_loss: 1.1207 - val_accuracy: 0.8130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ### 손실과 정확도 곡선 시각화"
      ],
      "metadata": {
        "id": "qQJukpoY9Suy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "KzRdA3Ig9c16",
        "outputId": "51e9731e-4be6-46da-94a0-3d356dbd88b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()   # 그래프를 초기화합니다\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "LPuqNksn9ibx",
        "outputId": "36e114a0-1b14-41ff-a2fb-dc6c98204756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #### 해석\n",
        "  * 이 모델은 9번째 에포크 이후에 과대적합이 시작됩니다. 9번의 에포크로 새로운 모델을 훈련하고 테스트 세트에서 평가하겠습니다:"
      ],
      "metadata": {
        "id": "NG4gizn69rBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=9,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "\n",
        "results = model.evaluate(x_test, one_hot_test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4ncjVCC96jS",
        "outputId": "30acc504-e69a-4ac6-daf1-fd873b26f614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9\n",
            "16/16 [==============================] - 2s 52ms/step - loss: 2.6838 - accuracy: 0.5103 - val_loss: 1.7921 - val_accuracy: 0.6150\n",
            "Epoch 2/9\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 1.4921 - accuracy: 0.6898 - val_loss: 1.3418 - val_accuracy: 0.7150\n",
            "Epoch 3/9\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 1.0961 - accuracy: 0.7648 - val_loss: 1.1445 - val_accuracy: 0.7560\n",
            "Epoch 4/9\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.8547 - accuracy: 0.8170 - val_loss: 1.0506 - val_accuracy: 0.7820\n",
            "Epoch 5/9\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.6749 - accuracy: 0.8577 - val_loss: 0.9610 - val_accuracy: 0.8020\n",
            "Epoch 6/9\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.5410 - accuracy: 0.8885 - val_loss: 0.9318 - val_accuracy: 0.8120\n",
            "Epoch 7/9\n",
            "16/16 [==============================] - 1s 42ms/step - loss: 0.4320 - accuracy: 0.9118 - val_loss: 0.8978 - val_accuracy: 0.8160\n",
            "Epoch 8/9\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.3536 - accuracy: 0.9287 - val_loss: 0.9043 - val_accuracy: 0.8220\n",
            "Epoch 9/9\n",
            "16/16 [==============================] - 1s 40ms/step - loss: 0.2898 - accuracy: 0.9395 - val_loss: 0.8814 - val_accuracy: 0.8220\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 0.9927 - accuracy: 0.7925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 무작위로 데이터셋을 분류해서 다시 실행\n",
        "  * 대략 78%의 정확도를 달성했습니다. 균형 잡힌 이진 분류 문제에서 완전히 무작위로 분류하면 50%의 정확도를 달성합니다.\n",
        "  * 이 문제는 불균형한 데이터셋을 사용하므로 무작위로 분류하면 19% 정도를 달성합니다. 여기에 비하면 이 결과는 꽤 좋은 편입니다:"
      ],
      "metadata": {
        "id": "IDJHgR6n-eH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "test_labels_copy = copy.copy(test_labels)\n",
        "\n",
        "np.random.shuffle(test_labels_copy)\n",
        "\n",
        "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrB4YCRe-Jzp",
        "outputId": "27aca61d-0dd5-4238-c306-b73ccfe8416b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18655387355298308"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # 새로운 데이터로 예측\n",
        "  * predict 메서드는 46개 토픽에 대한 확률 분포를 반환합니다. "
      ],
      "metadata": {
        "id": "Xhk8Zld7-pfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_test)"
      ],
      "metadata": {
        "id": "SL4hV-gr-1NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0].shape"
      ],
      "metadata": {
        "id": "fvoLWr5E-7d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693c416c-4421-474f-a58b-9bb44a1fb49b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46,)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLJ0chCH-_Q6",
        "outputId": "eeb7ad7b-f391-497d-96b4-b8ec72b4bac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 분류 결과\n",
        "  * 가장 큰 값이 예측 클래스가 됩니다. 즉, 가장 확률이 높은 클래스입니다:"
      ],
      "metadata": {
        "id": "sylkzDwx_A95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h0A5E9l_G6M",
        "outputId": "9a4c62ee-7fa7-48df-fb9a-d8393551f689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # 레이블 데이터 형태를 정수 텐서로 변환하는 방법으로 훈련\n",
        "  * 이 방식을 사용하려면 손실 함수 하나만 바꾸면 됩니다. 코드 3-21에 사용된 손실 함수 categorical_crossentropy는 레이블이 범주형 인코딩되어 있을 것이라고 기대합니다. 정수 레이블을 사용할 때는 sparse_categorical_crossentropy를 사용해야 합니다:"
      ],
      "metadata": {
        "id": "o6Keyg88Duuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)"
      ],
      "metadata": {
        "id": "hAXIE2pyD4MJ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop'\n",
        "              , loss='sparse_categorical_crossentropy'\n",
        "              , metrics=['acc'])"
      ],
      "metadata": {
        "id": "ryltui6MD73B"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # 충분히 큰 중간층을 두어야 하는 이유\n",
        "  * 앞서 언급한 것처럼 마지막 출력이 46차원이기 때문에 중간층의 히든 유닛이 46개보다 많이 적어서는 안 됩니다. 46차원보다 훨씬 작은 중간층(예를 들면 4차원)을 두면 정보의 병목이 어떻게 나타나는지 확인해 보겠습니다."
      ],
      "metadata": {
        "id": "-FC5JncHEP7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(4, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJHBeiMQEUK4",
        "outputId": "2228e7cb-d588-4fea-980d-6a3027ace9ac"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 2s 17ms/step - loss: 3.0847 - accuracy: 0.1998 - val_loss: 2.4327 - val_accuracy: 0.4490\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.9506 - accuracy: 0.4959 - val_loss: 1.7181 - val_accuracy: 0.5420\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.3371 - accuracy: 0.6937 - val_loss: 1.3288 - val_accuracy: 0.6840\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 1.0779 - accuracy: 0.7347 - val_loss: 1.2496 - val_accuracy: 0.7010\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.9597 - accuracy: 0.7486 - val_loss: 1.2259 - val_accuracy: 0.7000\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.8718 - accuracy: 0.7622 - val_loss: 1.2418 - val_accuracy: 0.7030\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.7988 - accuracy: 0.7823 - val_loss: 1.2267 - val_accuracy: 0.7150\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.7370 - accuracy: 0.8009 - val_loss: 1.2543 - val_accuracy: 0.7190\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.6781 - accuracy: 0.8208 - val_loss: 1.2612 - val_accuracy: 0.7280\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.6315 - accuracy: 0.8329 - val_loss: 1.2984 - val_accuracy: 0.7230\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 1s 15ms/step - loss: 0.5856 - accuracy: 0.8435 - val_loss: 1.3219 - val_accuracy: 0.7300\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.5492 - accuracy: 0.8485 - val_loss: 1.3406 - val_accuracy: 0.7280\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.5165 - accuracy: 0.8525 - val_loss: 1.4185 - val_accuracy: 0.7230\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.4887 - accuracy: 0.8589 - val_loss: 1.4882 - val_accuracy: 0.7140\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.4633 - accuracy: 0.8641 - val_loss: 1.5060 - val_accuracy: 0.7330\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.4400 - accuracy: 0.8750 - val_loss: 1.5624 - val_accuracy: 0.7280\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.4222 - accuracy: 0.8844 - val_loss: 1.6077 - val_accuracy: 0.7250\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.4010 - accuracy: 0.8916 - val_loss: 1.6986 - val_accuracy: 0.7250\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.3875 - accuracy: 0.8944 - val_loss: 1.7619 - val_accuracy: 0.7290\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 1s 14ms/step - loss: 0.3700 - accuracy: 0.8994 - val_loss: 1.8757 - val_accuracy: 0.7230\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fac23f02810>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## 결과\n",
        "  * 검증 정확도의 최고 값은 약 71%로 8% 정도 감소되었습니다. 이런 손실의 대부분 원인은 많은 정보(46개 클래스의 분할 초평면을 복원하기에 충분한 정보)를 중간층의 저차원 표현 공간으로 압축하려고 했기 때문입니다. 이 네트워크는 필요한 정보 대부분을 4차원 표현 안에 구겨 넣었지만 전부는 넣지 못했습니다."
      ],
      "metadata": {
        "id": "Y4gt7hz4EXu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VtnIL33_EepQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}